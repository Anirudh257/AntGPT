This fold contains the environment, data, scripts and results files for NeurIPS submission #3880: 
**Can Large Language Models Help Long-term Action Anticipation from Videos?**
In details, it covers the part using ICL and CoT for LTA tasks and some ablation experiments shown in table 3, 4 and figure 2, 3.

# Environment
```
pip install pandas editdistance rapidfuzz openai
```

# Dataset
+ `dicts.json`: dictionary file containing verb/noun to index dict and index to ver/noun dict.
+ `train_nseg8.jsonl`: training set with nseg=8, "prompt" correspond to ground-truth history actions, "completion" corresponds to ground-truth prediction.
+ `val_nseg8.jsonl`: validation set with nseg=8, "prompt" correspond to ground-truth history actions, "completion" corresponds to ground-truth prediction.
+ `val_nseg8_subset600.jsonl`: 600-sample subset of validation set, sampled from `val_nseg8.jsonl`.
+ `val_nseg8_recog.jsonl`: validation set with nseg=8, "prompt" correspond to history actions generated by action recognition model, "completion" corresponds to ground-truth prediction.
+ `val_nseg8_recog_subset600.jsonl`: 600-sample subset of validation set, sampled from `val_nseg8_recog.jsonl`.
+ `goal_val600.json`: Oracal goal generated by GPT-3.5-Turbo based on the recognized observation for the 600-sample validation subset.
+ `obj_val600.json`: Objsect information generated from ground-truth history nouns for the 600-sample validation subset.
+ `similar_matrix_subset600.pkl`: similarity matrix calculating the similarity between the whole training set and 600-sample validation set using the edit-distance between their history actions, used for post-processing.

# Scripts
For in-context-learning:
```
python query_icl.py --openai_key=<OPENAI_KEY>               # get ICL prediction based on history actions
python query_icl_obj.py --openai_key=<OPENAI_KEY>           # get ICL prediction based on history actions and objects 
python query_icl_goal.py --openai_key=<OPENAI_KEY>          # get ICL prediction based on history actions and goals
python query_icl_obj_goal.py --openai_key=<OPENAI_KEY>      # get ICL prediction based on history actions, objects, and goals

python eval_icl.py --response_name=<RESPONSE_FILE>      # post-process ICL response and calculate metrics, available RESPONSE_FILE is listed below.
```

For chain-of-thoughts:
```
python query_cot.py --openai_key=<OPENAI_KEY>           # get CoT prediction 
python eval_cot.py --response_name=<RESPONSE_FILE>      # post-process COT response and calculate metrics, available RESPONSE_FILE is listed below.
```


For goal geneartion:
```
python query_latent_goal.py --openai_key=<OPENAI_KEY>     # generate oracle goal
```

For counterfactual prediction: play with ``counterfactual.ipynb``.

# Existing Responses:
Existing responses are stored in `output/`:
For ICL evaluation:
```
RESPONSE_FILE = [
    subset600_responses_icl_recog.json,             # response file for ICL with recognition actions as input
    subset600_responses_icl_recog_obj.json,         # response file for ICL with recognition actions and objects as input
    subset600_responses_icl_recog_goal.json,        # response file for ICL with recognition actions and goals as input
    subset600_responses_icl_recog_obj_goal.json,    # response file for ICL with recognition actions and objects and goals as input
    subset600_responses_icl_gt.json,                # response file for ICL with ground-truth actions as input
]
```
For CoT evaluation:
```
RESPONSE_FILE = [
    subset600_responses_cot_recog.json,         # response file for CoT with recognition actions as input
    subset600_responses_cot_gt.json,            # response file for CoT with ground-truth actions as input
]
```